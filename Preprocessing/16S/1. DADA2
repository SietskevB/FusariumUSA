
# Need to rename sequencing files to import (later) in Qiime2
# Run all following commands, check the number 1671
# Final format should be like this: NS.1671.002.FLD_ill_170_i7---IDT_i5_5.F1D14_ITS2_L001_R1_001.fastq

rename 's/_I1/_R1/' *
rename 's/_I2/_R2/' *
rename  's/_R1/_L001_R1_001/' *
rename  's/_R2/_L001_R2_001/' *
rename  's/NS\.1671\.002\.FLD_ill_[0-9][0-9][0-9]i7---IDT_i5[0-9][0-9]\.//' *
rename  's/NS\.1671\.002\.FLD_ill_[0-9][0-9][0-9]i7---IDT_i5[0-9]\.//' *


############################################################################################################################


# Start in R
# To remove amplification primer sequences
# Note: samples demultiplexed by seq company, index primer seqs already removed
# Trim sequences based on quality scores
# And run a custom DADA2

# Use custom DADA2 because NovaSeq data has binned quality scores 
# This requires separate error learning prior to DADA2
# And custom error estimation in DADA2, which is not possible in the Qiime2 plugin

# General credit to B. Callahan's ITS pipeline workflow
# Many lines of code and clarifying comments in the workflow are his
# https://benjjneb.github.io/dada2/ITS_workflow.html

# R version 4.2.2

library("Matrix") # 1.5.4
library(ShortRead) # 1.54.0
library("dplyr") # 1.1.2
library(dada2) # 1.24.0
library(Biostrings) # 2.64.1
library("tidyverse") # 2.0.0
library("digest") # 0.6.31
library("phylotools") # 0.2.2


setwd("/yourdirectory/")


############################################################################################################################


path <- "/yourdirectory/Seqfiles" # CHANGE ME to the directory containing the fastq files after unzipping
# list.files(path)

# Forward and reverse fastq filenames have format: SAMPLENAME_R1_001.fastq and SAMPLENAME_R2_001.fastq
fnFs <- sort(list.files(path, pattern="_R1_001.fastq", full.names = TRUE))
fnRs <- sort(list.files(path, pattern="_R2_001.fastq", full.names = TRUE))


# Define primer sequences
FWD <- "CCTACGGGNGGCWGCAG"  
REV <- "GACTACHVGGGTATCTAATCC"  

# Check orientation of primer sequences in data sequences
allOrients <- function(primer) {
    # Create all orientations of the input sequence
    require(Biostrings)
    dna <- DNAString(primer)
    orients <- c(Forward = dna, Complement = Biostrings::complement(dna), Reverse = reverse(dna), 
        RevComp = reverseComplement(dna))
    return(sapply(orients, toString))
}

FWD.orients <- allOrients(FWD)
REV.orients <- allOrients(REV)
FWD.orients

# The presence of ambiguous bases (Ns) in the sequencing reads makes accurate mapping of short primer sequences difficult
# Next we are going to “pre-filter” the sequences just to remove those with Ns, but perform no other filtering

fnFs.filtN <- file.path(path, "filtN", basename(fnFs)) # Put N-filterd files in filtN/ subdirectory
fnRs.filtN <- file.path(path, "filtN", basename(fnRs))
filterAndTrim(fnFs, fnFs.filtN, fnRs, fnRs.filtN, maxN = 0, multithread = TRUE)

# Identify and count primer sequences in fw and rv reads
# Using one set of paired-end FASTQ files
primerHits <- function(primer, fn) {
    # Counts number of reads in which the primer is found
    nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
    return(sum(nhits > 0))
}

rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.filtN[[1]]), 
    FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.filtN[[1]]), 
    REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.filtN[[1]]), 
    REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.filtN[[1]]))
	
	
# If all is well, remove primer sequences with cutadapt

cutadapt <- "/opt/conda/anaconda3/envs/cutadaptenv/bin/cutadapt" # CHANGE ME to the cutadapt path on your machine
system2(cutadapt, args = "--version") # Run shell commands from R 
# If the above command succesfully executed, R has found cutadapt and you are ready to continue

# We now create output filenames for the cutadapt-ed files, and define the parameters we are going to give the cutadapt command
# The critical parameters are the primers, and they need to be in the right orientation
# i.e. the FWD primer should have been matching the forward-reads in its forward orientation, and the REV primer should have been matching the reverse-reads in its forward orientation

path.cut <- file.path(path, "cutadapt")
if(!dir.exists(path.cut)) dir.create(path.cut)
fnFs.cut <- file.path(path.cut, basename(fnFs))
fnRs.cut <- file.path(path.cut, basename(fnRs))

FWD.RC <- dada2:::rc(FWD)
REV.RC <- dada2:::rc(REV)

# Trim FWD and the reverse-complement of REV off of R1 (forward reads)
R1.flags <- paste("-g", FWD, "-a", REV.RC) 

# Trim REV and the reverse-complement of FWD off of R2 (reverse reads)
R2.flags <- paste("-G", REV, "-A", FWD.RC) 


# Run Cutadapt
for(i in seq_along(fnFs)) {
  system2(cutadapt, args = c(R1.flags, R2.flags, "-n", 2, # -n 2 required to remove FWD and REV from reads
                             "-o", fnFs.cut[i], "-p", fnRs.cut[i], # output files
                             fnFs.filtN[i], fnRs.filtN[i])) # input files
}

# Sanity check: count the presence of primers in the first cutadapt-ed sample
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.cut[[1]]), 
    FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.cut[[1]]), 
    REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.cut[[1]]), 
    REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.cut[[1]]))
# Should be all zeroes aka no primer sequences present any more


############################################################################################################################


# Read in the names of the cutadapt-ed FASTQ files and apply some string manipulation to get the matched lists of forward and reverse fastq files
# Forward and reverse fastq filenames have the format:
cutFs <- sort(list.files(path.cut, pattern="_R1_001.fastq", full.names = TRUE))
cutRs <- sort(list.files(path.cut, pattern="_R2_001.fastq", full.names = TRUE))

# Check quality profile of sequences
rand_samples <- sample(size = 20, 1:length(cutFs)) # grab 20 random samples to plot; change to do the same from cutRs

pdf("quality-16S-FW.pdf") 
plotQualityProfile(cutFs[rand_samples])
dev.off() 

pdf("USFields-16S-RV.pdf") 
plotQualityProfile(cutRs[rand_samples])
dev.off()


############################################################################################################################


# Filter and trim
# Assigning the filenames for the output of the filtered reads to be stored as fastq.gz files
filtFs <- file.path(path.cut, "filtered", basename(cutFs))
filtRs <- file.path(path.cut, "filtered", basename(cutRs))

# Filtering
# Very basic settings, very similar to standard settings used in Qiime2 DADA2
out <- filterAndTrim(cutFs, filtFs, cutRs, filtRs, 
	truncLen=c(225,220), maxN = 0, maxEE = c(2, 2), truncQ = 2, rm.phix = TRUE, compress = TRUE, multithread = TRUE) # multithread = FALSE in local environment

# Check output
head(out)


############################################################################################################################


# After trimming and filtering
path.filt <- file.path(path.cut, "filtered")

# Learn error rates
# Default nbases 
# Altering errorEstimationFunction
# Monotonicity enforced inside errorEstimationFunction (hhollandmoritz https://github.com/benjjneb/dada2/issues/1307#issuecomment-957680971)
# Use loessErrfun_modjona_mono according to jonalim @ GitHub page above
# Function at bottom of this part in code

errF <- learnErrors(filtFs, nbases = 1e+08, errorEstimationFunction = loessErrfun_modjona_mono, multithread=TRUE, verbose = TRUE)
errR <- learnErrors(filtRs, nbases = 1e+08, errorEstimationFunction = loessErrfun_modjona_mono, multithread=TRUE, verbose = TRUE)

# Save the error rates for later use
saveRDS(errF, "/yourdirectory/errF.rds")
saveRDS(errR, "/yourdirectory/errR.rds")

# Plot the error rates to check
# Default nbases
pdf("errF.pdf") 
plotErrors(errF, nominalQ = TRUE)
dev.off() 

pdf("errR.pdf") 
plotErrors(errR, nominalQ = TRUE)
dev.off()
	

# DADA2 
# Error rate: default nbases, altering errorEstimationFunction (loessErrfun_modjona)

# Forward reads
dadaFs <- dada(
  derep = filtFs,
  err = errF,
  selfConsist = TRUE,
  pool = FALSE,
  errorEstimationFunction = loessErrfun_modjona_mono, # function at end of this file
  multithread = TRUE, 
  verbose = TRUE
)

# Reverse reads
dadaRs <- dada(
  derep = filtRs,
  err = errR,
  selfConsist = TRUE,
  pool = FALSE,
  errorEstimationFunction = loessErrfun_modjona_mono, # function at end of this file
  multithread = TRUE, 
  verbose = TRUE
)


# Plot errors from DADA2 incl. monotonicity
pdf("dada2_errF.pdf") 
plotErrors(getErrors(dadaFs), nominalQ=TRUE)
dev.off()

pdf("dada2_errR.pdf") 
plotErrors(getErrors(dadaRs), nominalQ=TRUE)
dev.off()


############################################################################################################################


# Merge fw and rv reads
mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE)
# Inspect the merger data.frame from the first sample
head(mergers[[1]])

# Create feature table
seqtab <- makeSequenceTable(mergers)
# dim(seqtab)
# 287 samples, 1783335 features

# Inspect distribution of sequence lengths
# table(nchar(getSequences(seqtab)))
# Sequences are 225 - 433 in length

# Remove chimeras
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
# Identified 1442250 bimeras out of 1783335 input sequences.

# dim(seqtab.nochim)
# 287 samples, 341085 non-chimeric features

# sum(seqtab.nochim)/sum(seqtab)
# 0.75096 = frequency of non-chimeric merged sequences in the dataset

# Final check: number of reads that made it through each step in the pipeline from DADA2
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))
# If processing a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names
head(track)

write.csv(track, file = "USFall-16S-DADA2.csv")


############################################################################################################################


# Calculate MD5 sums for feature IDs
seqtab.col <- colnames(seqtab.nochim)

seqtab.hash <- data.frame(matrix(, nrow = 341085, ncol = 2))
colnames(seqtab.hash) <- c("sequence", "hash")
seqtab.hash$sequence <- seqtab.col

for (i in 1:length(seqtab.col)) {
seqtab.hash$hash[i] <- digest(seqtab.col[i], algo = "md5")
}

# Export filtered, merged, non-chimera sequences
# With old sequence names
uniquesToFasta(seqtab.nochim, fout="USFall-16S-repseqs.fasta", ids=colnames(seqtab.nochim))

# Replace sequence names in .fasta to hash
rename.fasta(infile = "USFall-16S-repseqs.fasta", ref_table = seqtab.hash, outfile = "USFall-16S-seq-hash.fasta")


# Add hash codes to feature table
seqtab.nochim.hash <- t(seqtab.nochim)
rownames(seqtab.nochim.hash) <- seqtab.hash$hash[which(rownames(seqtab.nochim.hash) == seqtab.hash$sequence)]

# With hash names
write.table(seqtab.nochim.hash, "USFall-16S-featuretable-hash.txt", sep="\t", row.names=TRUE, col.names=NA, quote=FALSE)

# Export table with sequences and hash ID codes
write.csv(seqtab.hash, file = "USFall-16S-seq-hash-match.csv")


############################################################################################################################


# Custom loessErrfun() 
# Monotonicity enforced inside function
loessErrfun_modjona_mono <- function(trans) {
  qq <- as.numeric(colnames(trans))
  est <- matrix(0, nrow=0, ncol=length(qq))
  for(nti in c("A","C","G","T")) {
    for(ntj in c("A","C","G","T")) {
      if(nti != ntj) {
        errs <- trans[paste0(nti,"2",ntj),]
        tot <- colSums(trans[paste0(nti,"2",c("A","C","G","T")),])
        rlogp <- log10((errs+1)/tot)  # 1 psuedocount for each err, but if tot=0 will give NA
        rlogp[is.infinite(rlogp)] <- NA
        df <- data.frame(q=qq, errs=errs, tot=tot, rlogp=rlogp)
        
        # original
        # ###! mod.lo <- loess(rlogp ~ q, df, weights=errs) ###!
        # mod.lo <- loess(rlogp ~ q, df, weights=tot) ###!
        # #        mod.lo <- loess(rlogp ~ q, df)
        
        # jonalim's solution
        # https://github.com/benjjneb/dada2/issues/938
        mod.lo <- loess(rlogp ~ q, df, weights = log10(tot),degree = 1, span = 0.95)
        
        pred <- predict(mod.lo, qq)
        maxrli <- max(which(!is.na(pred)))
        minrli <- min(which(!is.na(pred)))
        pred[seq_along(pred)>maxrli] <- pred[[maxrli]]
        pred[seq_along(pred)<minrli] <- pred[[minrli]]
        est <- rbind(est, 10^pred)
      } # if(nti != ntj)
    } # for(ntj in c("A","C","G","T"))
  } # for(nti in c("A","C","G","T"))
  
  # HACKY
  MAX_ERROR_RATE <- 0.25
  MIN_ERROR_RATE <- 1e-7
  est[est>MAX_ERROR_RATE] <- MAX_ERROR_RATE
  est[est<MIN_ERROR_RATE] <- MIN_ERROR_RATE
  
  # enforce monotonicity
  # https://github.com/benjjneb/dada2/issues/791
  estorig <- est
  est <- est %>%
    data.frame() %>%
    mutate_all(funs(case_when(. < X40 ~ X40,
                              . >= X40 ~ .))) %>% as.matrix()
  rownames(est) <- rownames(estorig)
  colnames(est) <- colnames(estorig)
  
  # Expand the err matrix with the self-transition probs
  err <- rbind(1-colSums(est[1:3,]), est[1:3,],
               est[4,], 1-colSums(est[4:6,]), est[5:6,],
               est[7:8,], 1-colSums(est[7:9,]), est[9,],
               est[10:12,], 1-colSums(est[10:12,]))
  rownames(err) <- paste0(rep(c("A","C","G","T"), each=4), "2", c("A","C","G","T"))
  colnames(err) <- colnames(trans)
  # Return
  return(err)
}
